{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819f8f0c",
   "metadata": {},
   "source": [
    "# Cryptocurrency Market Analysis: Granger Causality Testing\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive statistical analysis to identify causal relationships between different cryptocurrency pairs using Granger causality tests. The analysis helps understand which cryptocurrencies lead or lag in price movements, providing insights into market dynamics and information flow.\n",
    "\n",
    "## Background\n",
    "**Granger Causality** is a statistical hypothesis test for determining whether one time series is useful in forecasting another. If variable X \"Granger-causes\" variable Y, past values of X contain information that helps predict Y above and beyond the information contained in past values of Y alone.\n",
    "\n",
    "## Analysis Objectives\n",
    "1. **Data Preparation**: Load and align cryptocurrency price data across multiple trading pairs\n",
    "2. **Stationarity Testing**: Verify that log returns are stationary using ADF and KPSS tests\n",
    "3. **Lag Selection**: Determine optimal lag structure using information criteria (AIC, BIC)\n",
    "4. **Causality Testing**: Perform bidirectional Granger causality tests between BTC and other cryptocurrencies\n",
    "5. **Results Interpretation**: Identify which cryptocurrencies lead or lag in market movements\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller, kpss\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "raw_folder = Path.cwd().parent / \"data\" / \"raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d234b9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Loading Functions\n",
    "\n",
    "The data is stored in partitioned Parquet files for efficient storage and retrieval. Each cryptocurrency has multiple part files that need to be combined into a single DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_parts_to_df(folder):\n",
    "\t\"\"\"Load all parquet files from a folder and combine them into a single DataFrame.\n",
    "\t\n",
    "\tThis function scans a directory for partitioned parquet files (named part*.parquet),\n",
    "\tloads them sequentially with memory management, and combines them into a single\n",
    "\tDataFrame. It's designed to handle large datasets that are split across multiple files.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tfolder: Path object pointing to the folder containing parquet files\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t\tCombined DataFrame from all parquet files with all records concatenated\n",
    "\t\t\n",
    "\tRaises:\n",
    "\t\tFileNotFoundError: If no parquet files are found in the specified folder\n",
    "\t\t\n",
    "\tNote:\n",
    "\t\tThe function uses garbage collection after each file load to manage memory\n",
    "\t\tefficiently, which is important when working with large datasets.\n",
    "\t\"\"\"\n",
    "\tparts = sorted(folder.glob(\"part*.parquet\"))\n",
    "\t\n",
    "\tif not parts:\n",
    "\t\traise FileNotFoundError(f\"No parquet files found in {folder}\")\n",
    "\t\n",
    "\tprint(f\"Loading {len(parts)} parquet files from {folder.name}...\")\n",
    "\t\n",
    "\tdfs = []\n",
    "\tfor p in parts:\n",
    "\t\tdf = pd.read_parquet(p, engine=\"pyarrow\")\n",
    "\t\tdfs.append(df)\n",
    "\t\tgc.collect()\n",
    "\t\n",
    "\tcombined_df = pd.concat(dfs, ignore_index=True)\n",
    "\tdel dfs\n",
    "\tgc.collect()\n",
    "\t\n",
    "\tprint(f\"  ✓ Loaded {len(combined_df):,} records\")\n",
    "\treturn combined_df.iloc[-2_000_000:-250_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d80de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Statistical Testing Functions\n",
    "\n",
    "This section defines functions for various statistical tests required for time series analysis.\n",
    "\n",
    "### 3.1 Stationarity Testing\n",
    "\n",
    "Before performing Granger causality tests, we must ensure that our time series are stationary. Non-stationary data can lead to spurious regression results. We use two complementary tests:\n",
    "\n",
    "- **ADF (Augmented Dickey-Fuller)**: Tests the null hypothesis that a unit root is present (non-stationary). We want to reject this null (p < 0.05).\n",
    "- **KPSS (Kwiatkowski-Phillips-Schmidt-Shin)**: Tests the null hypothesis that the series is stationary. We want to fail to reject this null (p > 0.05).\n",
    "\n",
    "A series is considered stationary when both tests agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(series, name):\n",
    "\t\"\"\"Test for stationarity using ADF and KPSS tests.\n",
    "\t\n",
    "\tThis function performs two complementary stationarity tests to ensure the time\n",
    "\tseries doesn't have trends or seasonality that could lead to spurious results.\n",
    "\tThe ADF test checks for unit roots while KPSS tests for trend stationarity.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tseries: Time series data to test, typically log returns\n",
    "\t\tname: Name of the series for display purposes in the output\n",
    "\t\t\n",
    "\tNote:\n",
    "\t\t- ADF Test: H0 = non-stationary (reject if p < 0.05 means stationary)\n",
    "\t\t- KPSS Test: H0 = stationary (reject if p < 0.05 means non-stationary)\n",
    "\t\t- Ideal result: ADF p < 0.05 AND KPSS p > 0.05\n",
    "\t\"\"\"\n",
    "\tadf_p = adfuller(series.dropna())[1]\n",
    "\tkpss_p = kpss(series.dropna(), regression='c', nlags='auto')[1]\n",
    "\t\n",
    "\tadf_status = \"✓ Stationary\" if adf_p < 0.05 else \"✗ Non-stationary\"\n",
    "\tkpss_status = \"✓ Stationary\" if kpss_p > 0.05 else \"✗ Non-stationary\"\n",
    "\t\n",
    "\tprint(f\"{name:10s} | ADF p={adf_p:.4f} ({adf_status:10s}) | KPSS p={kpss_p:.4f} ({kpss_status})\")\n",
    "\n",
    "    \n",
    "def select_optimal_lag(df_combined, maxlag=10):\n",
    "\t\"\"\"Select optimal lag order for VAR model using information criteria.\n",
    "\t\n",
    "\tThis function fits Vector Autoregression (VAR) models with different lag orders\n",
    "\tand compares them using information criteria. Lower values of AIC/BIC indicate\n",
    "\tbetter model fit while penalizing model complexity.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tdf_combined: Combined DataFrame with multiple time series columns\n",
    "\t\tmaxlag: Maximum number of lags to test (default: 10)\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t\tOptimal lag order based on Akaike Information Criterion (AIC)\n",
    "\t\t\n",
    "\tNote:\n",
    "\t\tCommon information criteria:\n",
    "\t\t- AIC: Akaike Information Criterion (tends to select more lags)\n",
    "\t\t- BIC: Bayesian Information Criterion (more conservative, fewer lags)\n",
    "\t\t- HQIC: Hannan-Quinn Information Criterion (middle ground)\n",
    "\t\"\"\"\n",
    "\tmodel = VAR(df_combined)\n",
    "\tresult = model.select_order(maxlags=maxlag)\n",
    "\tprint(\"\\n--- VAR Lag Order Selection ---\")\n",
    "\tprint(result.summary())\n",
    "\treturn result.aic\n",
    "\n",
    "\n",
    "def johansen_test(df):\n",
    "\t\"\"\"Perform Johansen cointegration test.\n",
    "\t\n",
    "\tThe Johansen test determines whether multiple non-stationary time series have\n",
    "\ta long-run equilibrium relationship (cointegration). If series are cointegrated,\n",
    "\tthey move together in the long run despite short-term deviations.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tdf: DataFrame with multiple time series columns to test for cointegration\n",
    "\t\t\n",
    "\tNote:\n",
    "\t\t- If trace statistic > critical value, reject null of no cointegration\n",
    "\t\t- Cointegration suggests using VECM instead of VAR model\n",
    "\t\t- det_order=0 means no deterministic trend in the data\n",
    "\t\"\"\"\n",
    "\tresult = coint_johansen(df, det_order=0, k_ar_diff=1)\n",
    "\tprint(\"\\n--- Johansen Cointegration Test ---\")\n",
    "\tprint(\"Trace Statistics:\", result.lr1)\n",
    "\tprint(\"Critical Values:\", result.cvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cba82",
   "metadata": {},
   "source": [
    "### 3.2 Granger Causality Analysis\n",
    "\n",
    "The core analysis function that performs bidirectional Granger causality tests. This test determines whether past values of one time series help predict another time series better than using only the history of the predicted series itself.\n",
    "\n",
    "**Interpretation:**\n",
    "- X Granger-causes Y: Past values of X improve predictions of Y\n",
    "- Bidirectional causality: Both X→Y and Y→X are significant\n",
    "- No causality: Neither direction shows significant predictive power\n",
    "\n",
    "**Important Notes:**\n",
    "- \"Granger causality\" is about predictive power, not true causation\n",
    "- Requires stationary time series (hence we use log returns)\n",
    "- Results depend on the chosen lag order\n",
    "- Significance at α=0.05 level is typically used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7965c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_symbols_by_granger_causality(main_df, main_symbol, compared_df, compared_symbol, maxlag=5):\n",
    "\t\"\"\"Perform bidirectional Granger causality tests between two cryptocurrency pairs.\n",
    "\t\n",
    "\tThis comprehensive function handles the complete workflow for testing Granger causality\n",
    "\tbetween two cryptocurrencies:\n",
    "\t1. Data preparation and timestamp alignment\n",
    "\t2. Calculation of log returns for stationarity\n",
    "\t3. Stationarity testing with ADF and KPSS\n",
    "\t4. Optimal lag selection using VAR model\n",
    "\t5. Bidirectional Granger causality tests\n",
    "\t\n",
    "\tThe function tests both directions:\n",
    "\t- Does main_symbol Granger-cause compared_symbol? (e.g., BTC → ETH)\n",
    "\t- Does compared_symbol Granger-cause main_symbol? (e.g., ETH → BTC)\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tmain_df: DataFrame for the main symbol containing OHLCV data\n",
    "\t\tmain_symbol: Name of the main symbol (e.g., 'BTCUSDT')\n",
    "\t\tcompared_df: DataFrame for the compared symbol containing OHLCV data\n",
    "\t\tcompared_symbol: Name of the compared symbol (e.g., 'ETHUSDT')\n",
    "\t\tmaxlag: Maximum lag to test (default: 5). Higher lags test longer-term relationships\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t\tDictionary with two keys, each containing a list of (F-statistic, p-value) tuples:\n",
    "\t\t- '{main_symbol}_affects_{compared_symbol}': Results for main → compared direction\n",
    "\t\t- '{compared_symbol}_affects_{main_symbol}': Results for compared → main direction\n",
    "\t\tEach list has `maxlag` elements, one for each lag tested.\n",
    "\t\t\n",
    "\tRaises:\n",
    "\t\tKeyError: If 'close' column is missing from either DataFrame\n",
    "\t\tValueError: If insufficient observations remain after alignment and cleaning\n",
    "\t\tAssertionError: If timestamps don't align properly or aren't monotonically increasing\n",
    "\t\t\n",
    "\tExample:\n",
    "\t\t>>> result = compare_symbols_by_granger_causality(\n",
    "\t\t...     btc_df, \"BTCUSDT\", eth_df, \"ETHUSDT\", maxlag=3\n",
    "\t\t... )\n",
    "\t\t>>> # Check if BTC Granger-causes ETH at lag 1\n",
    "\t\t>>> f_stat, p_value = result['BTCUSDT_affects_ETHUSDT'][0]\n",
    "\t\t>>> if p_value < 0.05:\n",
    "\t\t...     print(\"BTC significantly Granger-causes ETH at lag 1\")\n",
    "\t\t\n",
    "\tNote:\n",
    "\t\tThe function uses log returns (log(price_t / price_{t-1})) instead of raw prices\n",
    "\t\tto achieve stationarity. Infinite values from zero prices are removed automatically.\n",
    "\t\tA smaller p-value (< 0.05) indicates stronger evidence of Granger causality.\n",
    "\t\"\"\"\n",
    "\tdf1 = main_df.copy()\n",
    "\tdf2 = compared_df.copy()\n",
    "\t\n",
    "\tdf1[\"open_time\"] = pd.to_datetime(df1[\"open_time\"], utc=True, errors=\"coerce\")\n",
    "\tdf2[\"open_time\"] = pd.to_datetime(df2[\"open_time\"], utc=True, errors=\"coerce\")\n",
    "\t\n",
    "\tdf1 = df1.dropna(subset=[\"open_time\"]).set_index(\"open_time\").sort_index()\n",
    "\tdf2 = df2.dropna(subset=[\"open_time\"]).set_index(\"open_time\").sort_index()\n",
    "\t\n",
    "\tcommon_idx = df1.index.intersection(df2.index)\n",
    "\tdf1 = df1.loc[common_idx]\n",
    "\tdf2 = df2.loc[common_idx]\n",
    "\t\n",
    "\tassert df1.index.equals(df2.index), \\\n",
    "\t\tf\"Index mismatch: main[{df1.index.min()}..{df1.index.max()}], compared[{df2.index.min()}..{df2.index.max()}]\"\n",
    "\tassert df1.index.is_monotonic_increasing, \"Index must be monotonically increasing\"\n",
    "\t\n",
    "\tif \"close\" not in df1 or \"close\" not in df2:\n",
    "\t\traise KeyError(\"Both dataframes must contain 'close' column\")\n",
    "\t\n",
    "\tdf1 = df1[df1[\"close\"] > 0]\n",
    "\tdf2 = df2[df2[\"close\"] > 0]\n",
    "\tcommon_idx = df1.index.intersection(df2.index)\n",
    "\tdf1 = df1.loc[common_idx]\n",
    "\tdf2 = df2.loc[common_idx]\n",
    "\t\n",
    "\tmain_return = np.log(df1[\"close\"] / df1[\"close\"].shift(1))\n",
    "\tcompared_return = np.log(df2[\"close\"] / df2[\"close\"].shift(1))\n",
    "\t\n",
    "\t# print(f\"\\n{'='*80}\")\n",
    "\t# print(f\"Stationarity Tests: {main_symbol} vs {compared_symbol}\")\n",
    "\t# print(f\"{'='*80}\")\n",
    "\t# check_stationarity(main_return, f\"{main_symbol} return\")\n",
    "\t# check_stationarity(compared_return, f\"{compared_symbol} return\")\n",
    "\t\n",
    "\tdf_combined = pd.DataFrame({\n",
    "\t\t\"main_return\": main_return,\n",
    "\t\t\"compared_return\": compared_return\n",
    "\t}).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\t\n",
    "\t# print(f\"\\n{'='*80}\")\n",
    "\t# print(f\"Lag Selection: {main_symbol} vs {compared_symbol}\")\n",
    "\t# print(f\"{'='*80}\")\n",
    "\t# optimal_lag = select_optimal_lag(df_combined, maxlag=maxlag)\n",
    "\t# print(f\"Recommended lag based on AIC: {optimal_lag}\")\n",
    "\t\n",
    "\tn = len(df_combined)\n",
    "\tif n < 3:\n",
    "\t\traise ValueError(\"Not enough observations after alignment to run Granger causality tests\")\n",
    "\tuse_maxlag = int(min(maxlag, max(1, n - 2)))\n",
    "\t\n",
    "\tresult = {\n",
    "\t\tf\"{main_symbol}_affects_{compared_symbol}\": [],\n",
    "\t\tf\"{compared_symbol}_affects_{main_symbol}\": []\n",
    "\t}\n",
    "\t\n",
    "\tdef extract_ftest(res_dict):\n",
    "\t\t\"\"\"Extract F-test statistics from Granger test results.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tres_dict: Result dictionary from grangercausalitytests\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tTuple of (F-statistic, p-value) from the F-test\n",
    "\t\t\t\n",
    "\t\tRaises:\n",
    "\t\t\tTypeError: If result structure is unexpected\n",
    "\t\t\tRuntimeError: If no valid test results found\n",
    "\t\t\"\"\"\n",
    "\t\tif isinstance(res_dict, dict):\n",
    "\t\t\ttests = res_dict.get(0, {}) if 0 in res_dict else next(iter(res_dict.values()))\n",
    "\t\telif isinstance(res_dict, (list, tuple)):\n",
    "\t\t\ttests = res_dict[0]\n",
    "\t\telse:\n",
    "\t\t\traise TypeError(\"Unexpected result structure in grangercausalitytests\")\n",
    "\t\t\n",
    "\t\tfor k in (\"ssr_ftest\", \"params_ftest\", \"lrtest\", \"ssr_chi2test\"):\n",
    "\t\t\tif k in tests:\n",
    "\t\t\t\tvals = tests[k]\n",
    "\t\t\t\tif isinstance(vals, (list, tuple)) and len(vals) >= 2:\n",
    "\t\t\t\t\treturn float(vals[0]), float(vals[1])\n",
    "\t\traise RuntimeError(\"Could not extract test results\")\n",
    "\t\n",
    "\t# Perform Granger causality tests: compared → main\n",
    "\tres = grangercausalitytests(\n",
    "\t\tdf_combined[[\"compared_return\", \"main_return\"]],\n",
    "\t\tmaxlag=use_maxlag,\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\tfor i, (lag, res_dict) in enumerate(res.items()):\n",
    "\t\tassert lag == i + 1, f\"Lag mismatch: {lag} != {i+1}\"\n",
    "\t\tfval, pval = extract_ftest(res_dict)\n",
    "\t\tresult[f\"{compared_symbol}_affects_{main_symbol}\"].append((fval, pval))\n",
    "\t\n",
    "\t# Perform Granger causality tests: main → compared\n",
    "\tres = grangercausalitytests(\n",
    "\t\tdf_combined[[\"main_return\", \"compared_return\"]],\n",
    "\t\tmaxlag=use_maxlag,\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\tfor i, (lag, res_dict) in enumerate(res.items()):\n",
    "\t\tassert lag == i + 1, f\"Lag mismatch: {lag} != {i+1}\"\n",
    "\t\tfval, pval = extract_ftest(res_dict)\n",
    "\t\tresult[f\"{main_symbol}_affects_{compared_symbol}\"].append((fval, pval))\n",
    "\t\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_CANDLES = 300_000  # Number of recent candles to analyze\n",
    "MAX_LAG = 30\n",
    "\n",
    "MAIN_SYMBOL = \"BTCUSDT\"\n",
    "SYMBOLS_TO_COMPARE = [\"ETHUSDT\", \"BNBUSDT\", \"SOLUSDT\"]\n",
    "\n",
    "print(f\"Analysis Configuration:\")\n",
    "print(f\"  Main Symbol: {MAIN_SYMBOL}\")\n",
    "print(f\"  Comparison Symbols: {', '.join(SYMBOLS_TO_COMPARE)}\")\n",
    "print(f\"  Sample Size: {LAST_CANDLES:,} candles\")\n",
    "print(f\"  Maximum Lag: {MAX_LAG}\\n\")\n",
    "\n",
    "main_df = load_all_parts_to_df(raw_folder / MAIN_SYMBOL).tail(LAST_CANDLES)\n",
    "granger_results = {}\n",
    "for symbol_to_compare in SYMBOLS_TO_COMPARE:\n",
    "\tcompared_df = load_all_parts_to_df(raw_folder / symbol_to_compare).tail(LAST_CANDLES)\n",
    "\tgranger_results[symbol_to_compare] = compare_symbols_by_granger_causality(\n",
    "\t\tmain_df, MAIN_SYMBOL, compared_df, symbol_to_compare, maxlag=MAX_LAG\n",
    "\t)\n",
    "\n",
    "for i in range(MAX_LAG):\n",
    "\tfor symbol, results in granger_results.items():\n",
    "\t\tmain_to_comp = results[f\"{MAIN_SYMBOL}_affects_{symbol}\"][i]\n",
    "\t\tcomp_to_main = results[f\"{symbol}_affects_{MAIN_SYMBOL}\"][i]\n",
    "\t\tprint(f\"\\nLag {i+1}: {MAIN_SYMBOL} → {symbol}: F={main_to_comp[0]:.4f}, p={main_to_comp[1]:.4f}\")\n",
    "\t\tprint(f\"Lag {i+1}: {symbol} → {MAIN_SYMBOL}: F={comp_to_main[0]:.4f}, p={comp_to_main[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611f111",
   "metadata": {},
   "source": [
    "### 4. Visualization of Granger Causality Results\n",
    "\n",
    "The following visualization summarizes the Granger causality test results computed above.\n",
    "\n",
    "- Left: heatmap of p-values for each tested lag and direction (rows = directional tests, columns = lag).\n",
    "- Right: line plot of F-statistics across lags for each directional test.\n",
    "\n",
    "Significant p-values (e.g., < 0.05) indicate evidence that one series helps predict the other at that lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "pvals = {}\n",
    "fstats = {}\n",
    "max_tested_lag = 0\n",
    "for symbol, res in granger_results.items():\n",
    "    key1 = f\"{MAIN_SYMBOL}_affects_{symbol}\"\n",
    "    l1 = len(res[key1])\n",
    "    max_tested_lag = max(max_tested_lag, l1)\n",
    "\n",
    "directions = []\n",
    "for symbol, res in granger_results.items():\n",
    "    directions.append(f\"{MAIN_SYMBOL}->{symbol}\")\n",
    "\n",
    "pvals_df = pd.DataFrame(index=directions, columns=range(1, max_tested_lag+1), dtype=float)\n",
    "fvals_df = pd.DataFrame(index=directions, columns=range(1, max_tested_lag+1), dtype=float)\n",
    "\n",
    "for symbol, res in granger_results.items():\n",
    "    d1 = f\"{MAIN_SYMBOL}->{symbol}\"\n",
    "    vals1 = res[f\"{MAIN_SYMBOL}_affects_{symbol}\"]\n",
    "    for i in range(max_tested_lag):\n",
    "        if i < len(vals1):\n",
    "            f, p = vals1[i]\n",
    "            fvals_df.loc[d1, i+1] = f\n",
    "            pvals_df.loc[d1, i+1] = p\n",
    "        else:\n",
    "            fvals_df.loc[d1, i+1] = float('nan')\n",
    "            pvals_df.loc[d1, i+1] = float('nan')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(pvals_df.astype(float), cmap='viridis_r', cbar_kws={'label': 'p-value'}, annot=False)\n",
    "plt.title('Granger Causality p-values (rows: direction, cols: lag)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Direction')\n",
    "\n",
    "sig_mask = pvals_df.astype(float) < 0.05\n",
    "for (y, x), val in np.ndenumerate(sig_mask.values):\n",
    "    if val:\n",
    "        plt.gca().add_patch(plt.Rectangle((x, y), 1, 1, fill=False, edgecolor='red', lw=1.0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for idx in fvals_df.index:\n",
    "    plt.plot(fvals_df.columns, fvals_df.loc[idx], marker='o', label=idx)\n",
    "plt.title('F-statistics across lags')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('F-statistic')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a57c9",
   "metadata": {},
   "source": [
    "### 5. Additional tests: correlation, structural breaks, rank tests, wavelet coherence and FEVD\n",
    "\n",
    "This section implements the additional checks requested by the user:\n",
    "\n",
    "1. Correlation tests (Pearson, Spearman, Kendall)\n",
    "2. Structural break tests:\n",
    "   - Chow test (single known breakpoint)\n",
    "   - Bai–Perron (multiple breaks) using the `ruptures` package\n",
    "3. Mann–Whitney U test (non-parametric comparison of distributions)\n",
    "4. Wavelet coherence (using `pycwt`, if available)\n",
    "5. FEVD (Forecast Error Variance Decomposition) based on VAR\n",
    "\n",
    "The code cell below provides helper functions and executes these tests for `MAIN_SYMBOL` vs each symbol in `SYMBOLS_TO_COMPARE`. If optional libraries are missing, the code will notify and provide install hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb82497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(df, col='close'):\n",
    "    r = np.log(df[col] / df[col].shift(1))\n",
    "    return r.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def chow_test(df, x_col, y_col, t0_idx):\n",
    "    df1 = df.iloc[:t0_idx]\n",
    "    df2 = df.iloc[t0_idx:]\n",
    "    model_full = ols(f\"{y_col} ~ {x_col}\", data=df).fit()\n",
    "    model_1 = ols(f\"{y_col} ~ {x_col}\", data=df1).fit()\n",
    "    model_2 = ols(f\"{y_col} ~ {x_col}\", data=df2).fit()\n",
    "    ssr_full = float(np.sum(model_full.resid ** 2))\n",
    "    ssr1 = float(np.sum(model_1.resid ** 2))\n",
    "    ssr2 = float(np.sum(model_2.resid ** 2))\n",
    "    k = int(model_full.df_model) + 1\n",
    "    n = len(df)\n",
    "    if (n - 2 * k) <= 0:\n",
    "        raise ValueError(f\"Invalid degrees of freedom for Chow test: n={n}, k={k}\")\n",
    "    num = (ssr_full - (ssr1 + ssr2)) / k\n",
    "    den = (ssr1 + ssr2) / (n - 2 * k)\n",
    "    Fstat = num / den\n",
    "    p = 1 - stats.f.cdf(Fstat, k, n - 2 * k)\n",
    "    return float(Fstat), float(p)\n",
    "\n",
    "def bai_perron_breaks(series, n_bkps=3):\n",
    "    algo = rpt.Binseg(model='l2').fit(series.values)\n",
    "    breaks = algo.predict(n_bkps=n_bkps)\n",
    "    return breaks\n",
    "\n",
    "def wavelet_coherence(x, y, dt=1.0):\n",
    "    import pycwt as wavelet\n",
    "    WCT, aWCT, coi, freqs, sig = wavelet.wct(x, y, dt)\n",
    "    phase = np.angle(aWCT)\n",
    "    return WCT, phase, coi, freqs, sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in SYMBOLS_TO_COMPARE:\n",
    "    print('\\n' + '='*60)\n",
    "    print(f\"Testing: {MAIN_SYMBOL} vs {symbol}\")\n",
    "    df_main = load_all_parts_to_df(raw_folder / MAIN_SYMBOL).tail(LAST_CANDLES)\n",
    "    df_symb = load_all_parts_to_df(raw_folder / symbol).tail(LAST_CANDLES)\n",
    "    df_main['open_time'] = pd.to_datetime(df_main['open_time'], utc=True, errors='coerce')\n",
    "    df_symb['open_time'] = pd.to_datetime(df_symb['open_time'], utc=True, errors='coerce')\n",
    "    df_main = df_main.set_index('open_time').sort_index()\n",
    "    df_symb = df_symb.set_index('open_time').sort_index()\n",
    "    common_idx = df_main.index.intersection(df_symb.index)\n",
    "    df_main = df_main.loc[common_idx]\n",
    "    df_symb = df_symb.loc[common_idx]\n",
    "\n",
    "    r_main = compute_returns(df_main).rename(f\"{MAIN_SYMBOL}_return\")\n",
    "    r_symb = compute_returns(df_symb).rename(f\"{symbol}_return\")\n",
    "    df = pd.concat([r_main, r_symb], axis=1).dropna()\n",
    "    if df.empty:\n",
    "        print(f\"No overlapping data for {symbol}\")\n",
    "        continue\n",
    "\n",
    "    pear  = df.iloc[:, 0].corr(df.iloc[:, 1], method='pearson')\n",
    "    spear = df.iloc[:, 0].corr(df.iloc[:, 1], method='spearman')\n",
    "    kend  = df.iloc[:, 0].corr(df.iloc[:, 1], method='kendall')\n",
    "    print(f\"Correlations: Pearson={pear:.4f}, Spearman={spear:.4f}, Kendall={kend:.4f}\")\n",
    "\n",
    "    stat, p = mannwhitneyu(df.iloc[:, 0].values, df.iloc[:, 1].values, alternative='two-sided')\n",
    "    print(f\"Mann-Whitney U p={p:.4f}\")\n",
    "\n",
    "    t0 = len(df) // 2\n",
    "    Fchow, pchow = chow_test(df.reset_index(drop=True), df.columns[0], df.columns[1], t0)\n",
    "    print(f\"Chow test at midpoint: F={Fchow:.4f}, p={pchow:.4f}\")\n",
    "\n",
    "    breaks = bai_perron_breaks(df.iloc[:, 1], n_bkps=3)\n",
    "    print(f\"Detected breakpoints (indices): {breaks}\")\n",
    "\n",
    "    win = min(200, len(df))\n",
    "    res_wct = wavelet_coherence(df.iloc[-win:, 1].values, df.iloc[-win:, 0].values, dt=1)\n",
    "    if res_wct is not None:\n",
    "        WCT, phase, coi, freqs, sig = res_wct\n",
    "        T = np.arange(WCT.shape[1], dtype=float)\n",
    "        period = 1.0 / freqs\n",
    "        \n",
    "        if WCT.shape != (len(period), len(T)):\n",
    "            WCT = WCT.T\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        cs = plt.contourf(T, np.log2(period), WCT, levels=100)\n",
    "        \n",
    "        if sig.shape == WCT.shape:\n",
    "            plt.contour(T, np.log2(period), sig, levels=[1], colors='white', linewidths=1.2)\n",
    "        if len(coi) == len(T):\n",
    "            plt.fill_between(T, np.log2(coi), np.log2(period.max()), color='gray', alpha=0.3)\n",
    "               \n",
    "        plt.ylabel(\"log₂(Period)\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.title(\"Wavelet Coherence\")\n",
    "        plt.colorbar(cs, label=\"Coherence\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    model = VAR(df)\n",
    "    res = model.fit(maxlags=5)\n",
    "    if res is None:\n",
    "        raise RuntimeError(\"VAR result is not available\")\n",
    "\n",
    "    fevd = res.fevd(10)\n",
    "    fevd.plot()\n",
    "    fevd.summary()\n",
    "    frac_h1 = float(fevd.decomp[0, 1, 0])\n",
    "    print(\"FEVD -- fraction at step 1:\")\n",
    "    print(frac_h1)\n",
    "\n",
    "    irf = res.irf(10)\n",
    "    irf.plot(orth=False)\n",
    "    irf.plot_cum_effects(orth=False)\n",
    "    irf_h1 = float(irf.irfs[0, 1, 0])\n",
    "    print(\"IRF -- step 1:\")\n",
    "    print(irf_h1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.scatterplot(x=df.columns[0], y=df.columns[1], data=df, s=10)\n",
    "    plt.title('Scatter of returns')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.lineplot(data=df)\n",
    "    plt.title('Returns time series')\n",
    "    if len(breaks) > 0:\n",
    "        rpt.display(df.iloc[:, 1].values, breaks)\n",
    "        plt.title('Detected breaks')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No ruptures/plots available', ha='center')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
