{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9f942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cee33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"BTCUSDT\"\n",
    "downloaded_folder = Path.cwd().parent / \"data\" / \"downloaded\"\n",
    "raw_folder = Path.cwd().parent / \"data\" / \"raw\"\n",
    "raw_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b23d3",
   "metadata": {},
   "source": [
    "### Data Extracting\n",
    "\n",
    "This step unzips downloaded archives containing CSVs for the target symbol (BTCUSDT), splits the extracted rows into manageable chunks and saves each chunk as a Parquet file in the `raw` folder, then loads and concatenates the relevant Parquet parts for the requested date range so the data is ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e20bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 13 parts to /home/bloodrain/Desktop/Deep_Learning/Crypto_Market_Analyzer/data/raw\n"
     ]
    }
   ],
   "source": [
    "ROWS = 250_000\n",
    "buf = []\n",
    "buf_rows = 0\n",
    "part = 1\n",
    "\n",
    "cols = [\n",
    "\t\"open_time\",\n",
    "\t\"open\",\n",
    "\t\"high\",\n",
    "\t\"low\",\n",
    "\t\"close\",\n",
    "\t\"volume\",\n",
    "\t\"close_time\",\n",
    "\t\"quote_volume\",\n",
    "\t\"count\",\n",
    "\t\"taker_buy_volume\",\n",
    "\t\"taker_buy_quote_volume\",\n",
    "\t\"ignore\",\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "\t\"open_time\": \"int64\",\n",
    "\t\"open\": \"float64\",\n",
    "\t\"high\": \"float64\",\n",
    "\t\"low\": \"float64\",\n",
    "\t\"close\": \"float64\",\n",
    "\t\"volume\": \"float64\",\n",
    "\t\"close_time\": \"int64\",\n",
    "\t\"quote_volume\": \"float64\",\n",
    "\t\"count\": \"int64\",\n",
    "\t\"taker_buy_volume\": \"float64\",\n",
    "\t\"taker_buy_quote_volume\": \"float64\",\n",
    "\t\"ignore\": \"int64\",\n",
    "}\n",
    "\n",
    "def flush():\n",
    "\tglobal buf, buf_rows, part\n",
    "\tif buf_rows == 0:\n",
    "\t\treturn\n",
    "\tdf = pd.concat(buf, ignore_index=True)\n",
    "\ttable = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\tpq.write_table(table, raw_folder / f\"part{part:04d}.parquet\")\n",
    "\tbuf.clear()\n",
    "\tbuf_rows = 0\n",
    "\tpart += 1\n",
    "\n",
    "def convert_dtypes(df):\n",
    "\tfor col, dtype in dtypes.items():\n",
    "\t\tdf[col] = df[col].astype(dtype)\n",
    "\treturn df\n",
    "\n",
    "for zip_file in sorted(\n",
    "\tdownloaded_folder.glob(f\"{symbol}-*.zip\"),\n",
    "\tkey=lambda x: datetime.strptime(\"-\".join(x.stem.split(\"-\")[-3:]), \"%Y-%m-%d\"),\n",
    "):\n",
    "\twith zipfile.ZipFile(zip_file) as zf:\n",
    "\t\tfor name in sorted([n for n in zf.namelist() if n.lower().endswith(\".csv\")]):\n",
    "\t\t\twith zf.open(name) as f:\n",
    "\t\t\t\tfor chunk in pd.read_csv(\n",
    "\t\t\t\t\tf,\n",
    "\t\t\t\t\theader=None,\n",
    "\t\t\t\t\tnames=cols,\n",
    "\t\t\t\t\tchunksize=10_000\n",
    "\t\t\t\t):\n",
    "\t\t\t\t\tchunk = chunk[~chunk.iloc[:, 0].eq(chunk.columns[0])]\n",
    "\t\t\t\t\tchunk = convert_dtypes(chunk)\n",
    "\t\t\t\t\twhile len(chunk) > 0:\n",
    "\t\t\t\t\t\tneed = ROWS - buf_rows\n",
    "\t\t\t\t\t\ttake = chunk.iloc[:need]\n",
    "\t\t\t\t\t\tif len(take) > 0:\n",
    "\t\t\t\t\t\t\tbuf.append(take)\n",
    "\t\t\t\t\t\t\tbuf_rows += len(take)\n",
    "\t\t\t\t\t\tchunk = chunk.iloc[need:]\n",
    "\t\t\t\t\t\tif buf_rows == ROWS:\n",
    "\t\t\t\t\t\t\tflush()\n",
    "\tzip_file.unlink()\n",
    "\n",
    "flush()\n",
    "print(f\"Saved {part-1} parts to {raw_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f0756",
   "metadata": {},
   "source": [
    "### Resource usage\n",
    "\n",
    "Loads each Parquet part briefly to report on-disk size, the DataFrame's logical memory usage, and the process RSS delta incurred while reading the file. The printed per-part metrics (disk, logical_mem, rss_delta) and the derived \"overhead\" factor are intended to help you estimate the real memory cost of loading the dataset as a single DataFrame.\n",
    "\n",
    "Review the SUMMARY output to understand whether it is safe to load all parts at once or whether you should instead: process data in streaming/chunked fashion, filter or downcast columns, increase swap/VM resources, or use out-of-core tools (e.g., Dask, Vaex) to avoid exhausting system RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f178b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part0001.parquet: rows=250,000, disk=15.8MiB, logical_mem=22.9MiB, rss_delta=54.8MiB, overhead=2.40x\n",
      "part0002.parquet: rows=250,000, disk=15.6MiB, logical_mem=22.9MiB, rss_delta=63.6MiB, overhead=2.78x\n",
      "part0003.parquet: rows=250,000, disk=16.6MiB, logical_mem=22.9MiB, rss_delta=50.4MiB, overhead=2.20x\n",
      "part0004.parquet: rows=250,000, disk=16.6MiB, logical_mem=22.9MiB, rss_delta=52.6MiB, overhead=2.30x\n",
      "part0005.parquet: rows=250,000, disk=16.1MiB, logical_mem=22.9MiB, rss_delta=50.0MiB, overhead=2.18x\n",
      "part0006.parquet: rows=250,000, disk=14.4MiB, logical_mem=22.9MiB, rss_delta=50.6MiB, overhead=2.21x\n",
      "part0007.parquet: rows=250,000, disk=14.4MiB, logical_mem=22.9MiB, rss_delta=50.5MiB, overhead=2.21x\n",
      "part0008.parquet: rows=250,000, disk=13.7MiB, logical_mem=22.9MiB, rss_delta=44.8MiB, overhead=1.96x\n",
      "part0009.parquet: rows=250,000, disk=15.3MiB, logical_mem=22.9MiB, rss_delta=52.9MiB, overhead=2.31x\n",
      "part0010.parquet: rows=250,000, disk=14.7MiB, logical_mem=22.9MiB, rss_delta=47.2MiB, overhead=2.06x\n",
      "part0011.parquet: rows=250,000, disk=15.4MiB, logical_mem=22.9MiB, rss_delta=49.1MiB, overhead=2.15x\n",
      "part0012.parquet: rows=250,000, disk=15.1MiB, logical_mem=22.9MiB, rss_delta=47.0MiB, overhead=2.05x\n",
      "part0013.parquet: rows=39,840, disk=2.7MiB, logical_mem=3.6MiB, rss_delta=6.9MiB, overhead=1.88x\n",
      "\n",
      "SUMMARY\n",
      "parts: 13\n",
      "total rows: 3,039,840\n",
      "total disk size: 186.6MiB\n",
      "total logical mem: 278.3MiB\n",
      "total rss delta: 620.5MiB\n",
      "avg overhead factor: 2.21x\n",
      "avg mem per row: 214.0B/row\n",
      "estimated memory to load as one DataFrame (≈+10% overhead): 682.5MiB\n"
     ]
    }
   ],
   "source": [
    "def sizeof_fmt(num, suffix=\"B\"):\n",
    "\tfor unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\"]:\n",
    "\t\tif abs(num) < 1024.0:\n",
    "\t\t\treturn f\"{num:3.1f}{unit}{suffix}\"\n",
    "\t\tnum /= 1024.0\n",
    "\treturn f\"{num:.1f}Pi{suffix}\"\n",
    "\n",
    "def rss_bytes():\n",
    "\treturn psutil.Process(os.getpid()).memory_info().rss\n",
    "\n",
    "parts = sorted(raw_folder.glob(\"part*.parquet\"))\n",
    "if not parts:\n",
    "\tprint(f\"No parquet parts found in {raw_folder}\")\n",
    "else:\n",
    "\trows = []\n",
    "\tdisk_sizes = []\n",
    "\tlogical_mem = []\n",
    "\trss_deltas = []\n",
    "\toverheads = []\n",
    "\n",
    "\tfor p in parts:\n",
    "\t\tsize_on_disk = p.stat().st_size\n",
    "\t\tbefore = rss_bytes()\n",
    "\t\tdf = pd.read_parquet(p, engine=\"pyarrow\")\n",
    "\t\tafter = rss_bytes()\n",
    "\t\trow_count = len(df)\n",
    "\t\tmem_bytes = df.memory_usage(deep=True).sum()\n",
    "\t\tdelta_rss = max(after - before, 0)\n",
    "\t\toverhead = delta_rss / mem_bytes if mem_bytes else 0\n",
    "\n",
    "\t\trows.append(row_count)\n",
    "\t\tdisk_sizes.append(size_on_disk)\n",
    "\t\tlogical_mem.append(mem_bytes)\n",
    "\t\trss_deltas.append(delta_rss)\n",
    "\t\toverheads.append(overhead)\n",
    "\n",
    "\t\tprint(f\"{p.name}: rows={row_count:,}, disk={sizeof_fmt(size_on_disk)}, \"\n",
    "\t\t\t  f\"logical_mem={sizeof_fmt(mem_bytes)}, rss_delta={sizeof_fmt(delta_rss)}, \"\n",
    "\t\t\t  f\"overhead={overhead:,.2f}x\")\n",
    "\n",
    "\t\tdel df\n",
    "\t\tgc.collect()\n",
    "\n",
    "\ttotal_rows = sum(rows)\n",
    "\ttotal_disk = sum(disk_sizes)\n",
    "\ttotal_logical = sum(logical_mem)\n",
    "\ttotal_rss = sum(rss_deltas)\n",
    "\tavg_overhead = sum(overheads) / len(overheads) if overheads else 0\n",
    "\tavg_mem_per_row = total_rss / total_rows if total_rows else 0\n",
    "\test_total_single_df = int(total_rss * 1.10)\n",
    "\n",
    "\tprint(\"\\nSUMMARY\")\n",
    "\tprint(f\"parts: {len(parts)}\")\n",
    "\tprint(f\"total rows: {total_rows:,}\")\n",
    "\tprint(f\"total disk size: {sizeof_fmt(total_disk)}\")\n",
    "\tprint(f\"total logical mem: {sizeof_fmt(total_logical)}\")\n",
    "\tprint(f\"total rss delta: {sizeof_fmt(total_rss)}\")\n",
    "\tprint(f\"avg overhead factor: {avg_overhead:.2f}x\")\n",
    "\tprint(f\"avg mem per row: {sizeof_fmt(avg_mem_per_row)}/row\")\n",
    "\tprint(f\"estimated memory to load as one DataFrame (≈+10% overhead): {sizeof_fmt(est_total_single_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4837ec0",
   "metadata": {},
   "source": [
    "### Continuity checks for time series integrity\n",
    "\n",
    "Vrifies the 1-minute cadence of the `open_time` column across and between Parquet parts. It detects duplicates, non-monotonic steps, and missing rows by comparing successive timestamps. Use these diagnostics to identify gaps or anomalies that must be handled before feature engineering or model training (e.g., imputation, interpolation, or dropping corrupted rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e197ae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gaps or anomalies detected!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# verify continuity of open_time across parquet parts (1 minute = 60_000 ms)\n",
    "DELTA = 60_000\n",
    "\n",
    "issues = []\n",
    "prev_last = None\n",
    "\n",
    "for p in sorted(parts):\n",
    "\tdf = pd.read_parquet(p, engine=\"pyarrow\")\n",
    "\tdf = df.sort_values(\"open_time\").reset_index(drop=True)\n",
    "\n",
    "\tdiffs = df[\"open_time\"].diff()\n",
    "\tbad_idx = diffs[1:].ne(DELTA)\n",
    "\tif bad_idx.any():\n",
    "\t\tfor rel_i in bad_idx[bad_idx].index:\n",
    "\t\t\ti = rel_i  # already offset by 0 because we sliced [1:]\n",
    "\t\t\tprev_time = int(df.at[i - 1, \"open_time\"])\n",
    "\t\t\tcur_time = int(df.at[i, \"open_time\"])\n",
    "\t\t\tdiff = cur_time - prev_time\n",
    "\t\t\tif diff == 0:\n",
    "\t\t\t\tkind = \"duplicate\"\n",
    "\t\t\t\tmissing = 0\n",
    "\t\t\telif diff < DELTA:\n",
    "\t\t\t\tkind = \"non-monotonic/too-close\"\n",
    "\t\t\t\tmissing = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tkind = \"missing\"\n",
    "\t\t\t\tmissing = int(diff // DELTA) - 1\n",
    "\t\t\tissues.append({\n",
    "\t\t\t\t\"part\": p.name,\n",
    "\t\t\t\t\"type\": kind,\n",
    "\t\t\t\t\"prev_index\": i - 1,\n",
    "\t\t\t\t\"cur_index\": i,\n",
    "\t\t\t\t\"prev_time_ms\": prev_time,\n",
    "\t\t\t\t\"cur_time_ms\": cur_time,\n",
    "\t\t\t\t\"prev_time\": pd.to_datetime(prev_time, unit=\"ms\"),\n",
    "\t\t\t\t\"cur_time\": pd.to_datetime(cur_time, unit=\"ms\"),\n",
    "\t\t\t\t\"diff_ms\": diff,\n",
    "\t\t\t\t\"missing_rows\": missing,\n",
    "\t\t\t})\n",
    "\n",
    "\t# check boundary with previous part\n",
    "\tfirst_time = int(df.at[0, \"open_time\"])\n",
    "\tif prev_last is not None:\n",
    "\t\tif first_time != prev_last + DELTA:\n",
    "\t\t\tdiff = first_time - prev_last\n",
    "\t\t\tif diff == 0:\n",
    "\t\t\t\tkind = \"boundary_duplicate\"\n",
    "\t\t\t\tmissing = 0\n",
    "\t\t\telif diff < DELTA:\n",
    "\t\t\t\tkind = \"boundary_non_monotonic/too-close\"\n",
    "\t\t\t\tmissing = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tkind = \"boundary_missing\"\n",
    "\t\t\t\tmissing = int(diff // DELTA) - 1\n",
    "\t\t\tissues.append({\n",
    "\t\t\t\t\"part\": p.name,\n",
    "\t\t\t\t\"type\": kind,\n",
    "\t\t\t\t\"prev_part_last_time_ms\": prev_last,\n",
    "\t\t\t\t\"cur_part_first_time_ms\": first_time,\n",
    "\t\t\t\t\"prev_part_last_time\": pd.to_datetime(prev_last, unit=\"ms\"),\n",
    "\t\t\t\t\"cur_part_first_time\": pd.to_datetime(first_time, unit=\"ms\"),\n",
    "\t\t\t\t\"diff_ms\": diff,\n",
    "\t\t\t\t\"missing_rows\": missing,\n",
    "\t\t\t})\n",
    "\n",
    "\tprev_last = int(df.at[len(df) - 1, \"open_time\"])\n",
    "\n",
    "\tdel df\n",
    "\tgc.collect()\n",
    "\n",
    "# report\n",
    "if not issues:\n",
    "\tprint(\"No gaps or anomalies detected!\")\n",
    "else:\n",
    "\tprint(f\"Detected {len(issues)} issue(s). Sample:\")\n",
    "\tfor it in issues[:10]:\n",
    "\t\tit2 = it.copy()\n",
    "\t\tif \"prev_time\" in it2:\n",
    "\t\t\tit2[\"prev_time\"] = str(it2[\"prev_time\"])\n",
    "\t\tif \"cur_time\" in it2:\n",
    "\t\t\tit2[\"cur_time\"] = str(it2[\"cur_time\"])\n",
    "\t\tif \"prev_part_last_time\" in it2:\n",
    "\t\t\tit2[\"prev_part_last_time\"] = str(it2[\"prev_part_last_time\"])\n",
    "\t\tif \"cur_part_first_time\" in it2:\n",
    "\t\t\tit2[\"cur_part_first_time\"] = str(it2[\"cur_part_first_time\"])\n",
    "\t\tprint(json.dumps(it2, default=str, indent=2))\n",
    "\tissues_df = pd.DataFrame(issues)\n",
    "\tdisplay(issues_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
